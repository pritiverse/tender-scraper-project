Tender Scraper Project
ğŸ“Œ Overview

This project is a Scrapy-based web scraper designed to extract tender data from the Indian Central Public Procurement Portal
.

It collects:

Tender IDs

Titles

Issuing authorities

Dates

Other metadata

The scraped data is stored in a MongoDB database, making it ready for further analysis or API consumption.

âœ¨ Features

Scrapes tenders with structured data fields.

Supports pagination to capture multiple pages.

Converts dates to ISO 8601 format.

Stores data cleanly in MongoDB.

Easily extensible for other portals or extra tender details.

âš™ï¸ Installation
Prerequisites

Python 3.9+

MongoDB (local or remote)

Git (optional, for cloning the repo)

Setup Steps

Clone the repository:

git clone <repository_url>
cd tender-scraper-project


Create and activate a virtual environment:

python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate


Install dependencies:

pip install -r requirements.txt

ğŸ“‚ Project Structure
tender-scraper-project/
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tenderscraper/              # Scrapy project folder
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py                 # Data model
â”‚   â”œâ”€â”€ pipelines.py             # MongoDB pipeline
â”‚   â”œâ”€â”€ settings.py              # Scrapy & MongoDB config
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ cppp_spider.py       # Main spider (scraping + pagination)
â”œâ”€â”€ api/                         # (Optional) API backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ models.py
â””â”€â”€ run_api.py                   # API launch script

â–¶ï¸ Usage

From the project root (scrapy.cfg location), run:

scrapy crawl cppp


This will:

Start scraping from the first page.

Follow pagination links.

Stop after scraping 30 tenders (configurable).

Insert data into MongoDB â†’ tenderdb database â†’ tenders collection.

ğŸ§© Code Highlights

cppp_spider.py â†’ Spider with XPath extraction, pagination, and ISO date parsing.

pipelines.py â†’ Handles MongoDB storage.

settings.py â†’ MongoDB config + scraping parameters.

Pagination logic â†’ Automatically increments page queries.

Error handling â†’ Manages missing fields & date parsing issues.

ğŸš€ Improvements & Next Steps

Enhance XPath selectors to capture richer details (eligibility, value, category).

Follow tender detail pages for documents & in-depth info.

Normalize/validate all date & numerical fields.

Implement incremental scraping to avoid duplicates.

Add anti-blocking techniques (user-agent rotation, proxies).

Build a REST API for frontend consumption.

Automate runs via cron jobs / CI pipelines.

Extend to multi-country portals with configurable spiders.
