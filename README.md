```markdown
# Tender Scraper Project

ğŸ“Œ **Overview**  
This project is a Scrapy-based web scraper designed to extract tender data from the [Indian Central Public Procurement Portal](https://eprocure.gov.in/cppp/latestactivetendersnew/cpppdata?utm_source=chatgpt.com).  
It collects:  
- Tender IDs  
- Titles  
- Issuing authorities  
- Dates  
- Other metadata  

The scraped data is stored in a MongoDB database, making it ready for further analysis or API consumption.

---

âœ¨ **Features**  
- Scrapes tenders with structured data fields.  
- Supports pagination to capture multiple pages.  
- Converts dates to ISO 8601 format.  
- Stores data cleanly in MongoDB.  
- Easily extensible for other portals or extra tender details.

---

âš™ï¸ **Installation**

**Prerequisites:**  
- Python 3.9+  
- MongoDB (local or remote)  
- Git (optional, for cloning the repo)

**Setup Steps:**  
```
git clone <repository_url>
cd tender-scraper-project

python -m venv venv

# Windows
venv\Scripts\activate

# Linux/macOS
source venv/bin/activate

pip install -r requirements.txt
```

---

ğŸ“‚ **Project Structure**

```
tender-scraper-project/
â”œâ”€â”€ scrapy.cfg
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ tenderscraper/              # Scrapy project folder
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ items.py                 # Data model
â”‚   â”œâ”€â”€ pipelines.py             # MongoDB pipeline
â”‚   â”œâ”€â”€ settings.py              # Scrapy & MongoDB config
â”‚   â””â”€â”€ spiders/
â”‚       â”œâ”€â”€ __init__.py
â”‚       â””â”€â”€ cppp_spider.py       # Main spider (scraping + pagination)
â”œâ”€â”€ api/                         # (Optional) API backend
â”‚   â”œâ”€â”€ __init__.py
â”‚   â”œâ”€â”€ main.py
â”‚   â””â”€â”€ models.py
â””â”€â”€ run_api.py                   # API launch script
```

---

â–¶ï¸ **Usage**

From the project root (where `scrapy.cfg` is), run:  
```
scrapy crawl cppp
```

This will:  
- Start scraping from the first page.  
- Follow pagination links automatically.  
- Stop after scraping 30 tenders (configurable in spider).  
- Insert data into MongoDB â†’ `tenderdb` database â†’ `tenders` collection.

---

ğŸ§© **Code Highlights**

- `cppp_spider.py` â†’ Spider with XPath extraction, pagination, and ISO date parsing.  
- `pipelines.py` â†’ Handles MongoDB storage and upserts to avoid duplicates.  
- `settings.py` â†’ MongoDB connection config and Scrapy parameters.  
- Pagination logic â†’ Automatically increments page query parameters on portal.  
- Error handling â†’ Manages missing fields & date parsing issues gracefully.

---

ğŸš€ **Improvements & Next Steps**

ğŸ”¹ **Technology Stack & Anti-Scraping Mitigation**  
- Python (Scrapy) for scraping orchestration; fallback to Selenium/Playwright for dynamic content.  
- BeautifulSoup/lxml for advanced parsing as needed.  
- Proxy rotation and user-agent spoofing for stealth and robustness.  
- CAPTCHA solving if required for protected portals.

ğŸ”¹ **Vector Embedding Pipeline**  
- Scrape & Store â†’ Insert MongoDB document, `vectorEmbedding = null`.  
- Event Trigger â†’ Use RabbitMQ / Kafka or MongoDB Change Streams.  
- Embedding Worker â†’ Generate semantic text embeddings (Sentence-BERT / OpenAI API).  
- Update Record â†’ Patch `vectorEmbedding` field with float vector array.

ğŸ”¹ **Output Formatting & Consistency**  
- Maintain API and database document compatibility with shared specs.  
- Map unstructured data sections as raw text or HTML for full fidelity.  
- Test with synthetic data aligned with output schema.

---
